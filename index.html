<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>MAmmoTH2</title>
    <link rel="icon" type="image/x-icon" href="static/images/mammoth_icon.png">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/question_card.js"></script>
  <script src="./data/results/data_setting.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>
  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
        <a class="navbar-item" href="https://opencodeinterpreter.github.io/">
            <b>OpenCodeInterpreter</b> <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
            </a>
          <a class="navbar-item" href="https://mmmu-benchmark.github.io/">
            <b>MMMU</b> <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
          </a>
          <a class="navbar-item" href="https://tiger-ai-lab.github.io/MAmmoTH/">
            <b>MAmmoTH</b> <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
          </a>
          <a class="navbar-item" href="https://osu-nlp-group.github.io/TableLlama/">
            TableLlama 
            <a class="navbar-item" href="https://osu-nlp-group.github.io/MagicBrush/">
              MagicBrush
            </a>
            <a class="navbar-item" href="https://osu-nlp-group.github.io/Mind2Web/">
              Mind2Web
            </a>
          </a>
          
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">ðŸ¦£ MAmmoTH2: <br>Scaling Instructions from the Web</h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                <sup>â™¢</sup><a href="https://xiangyue9607.github.io/" target="_blank">Xiang Yue</a>,</span>
                            <span class="author-block">
                  <sup>â™ </sup><a href="https://scholar.google.com/citations?hl=zh-CN&user=hty-MWIAAAAJ" target="_blank">Tuney Zheng</a>,
                            <span class="author-block">
                    <sup>â™ </sup><a href="https://scholar.google.com/citations?user=qyTrq4kAAAAJ&hl=en" target="_blank">Ge Zhang</a>,
                            <span class="author-block">
                    <sup>â™ </sup><a href="https://wenhuchen.github.io/" target="_blank">Wenhu Chen</a>
                  </span>
                        </div>



                <div class="is-size-5 publication-authors">
                    <span class="author-block">
                        <sup>â™¢</sup>Carnegie Mellon University,
                        <sup>â™ </sup>University of Waterloo
                        <br>
                    <!-- <span class="eql-cntrb"><small><sup>*</sup>Xiang Yue and Wenhu Chen are the leading authors of the project. They contributed equally to this project.</small></span> -->
                            <span class="author-block"><a href="mailto:xyue2@andrew.cmu.edu">xyue2@andrew.cmu.edu</a>
                      , <a href="mailto:wenhuchen@uwaterloo.ca">wenhuchen@uwaterloo.ca</a> </span>

                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                          <a href="https://huggingface.co/datasets/" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            ðŸ¤—
                          </span>
                                <span>Dataset</span>
                                </a>
                                </span>

                                <!-- Supplementary PDF link -->
                                <span class="link-block">
                        <a href="https://huggingface.co/TIGER-Lab/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          ðŸ¤—
                        </span>
                                <span>Models</span>
                                </a>
                                </span>

                                <!-- Github link -->
                                <span class="link-block">
                      <a href="https://github.com/TIGER-AI-Lab/MAmmoTH2" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                                <span>Code</span>
                                </a>
                                </span>

                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                    <a href="https://arxiv.org/pdf/" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                                <span>arXiv</span>
                                </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <!-- Paper abstract -->
   <!-- Paper Abstract Section -->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Instruction tuning improves the reasoning abilities of large language models
(LLMs), with data quality and scalability being the crucial factors. Most instruction
tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a
paradigm to efficiently harvest 10 million naturally existing instruction data from
the pre-training web corpus to enhance LLM reasoning. Our approach involves
(1) recalling relevant documents, (2) extracting instruction-response pairs, and (3)
refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on
this dataset, we build MAmmoTH2 models, which significantly boost performance on
reasoning benchmarks. Notably, MAmmoTH2-7Bâ€™s (Mistral) performance increases
from 11% to 34% on MATH and from 36% to 67% on GSM8K without training
on any in-domain data. Further training MAmmoTH2 on public instruction tuning
datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several
reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-
scale, high-quality instruction data without costly human annotation or GPT-4
distillation, providing a new perspective on building better instruction tuning data.                        </p>

<p>
    <!-- Image1 carousel -->
        
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <div class="item">
                            <!-- Displaying an Image of Hybrid Instruction Tuning -->
                            <img src="static\images\teaser_front.jpg" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" />
                            <h2 class="subtitle" style="font-size: 16px; text-align: center; color: #888888;font-weight: normal;">
                                Figure 1: Overview of MAmmoTH2-Plus results. The MAmmoTH2-8x7B-Plus variant outperforms Mixtral-Instruct on reasoning benchmarks, matching Qwen-1.5-110B with only 13% active parame-ters. It also surpasses Mixtral-Instruct by around 10 points on general code and chatbot benchmarks.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    
    <!-- End image1 carousel -->
    
    <p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End of Paper Abstract Section -->





<!--begining introduction-->

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Added inline style for right alignment -->
                    <h2 class="title is-3">Introduction</h2>
                    <div class="content has-text-justified">
                        Reasoning is crucial for problem-solving and advancing knowledge. While large language models (LLMs) have made strides in natural language processing (NLP), their ability in complex reasoning tasks remains limited. Efforts to enhance LLMs' reasoning include continued training on filtered documents and instruction tuning with supervised fine-tuning loss. However, datasets for instruction tuning are often limited and biased, prompting a need for scalable and diverse instruction data.
<br>
<br>
                        To address this, we propose discovering instruction data from the web. We argue that vast amounts of high-quality instruction data exist in the web corpus, spanning various domains like math and science. Our three-step pipeline involves recalling documents from Common Crawl, extracting Q-A pairs, and refining them for quality. This approach yields 10 million instruction-response pairs, offering a scalable alternative to existing datasets.
<br>
<br>
                        Validating our dataset's effectiveness, we trained models on various base models, significantly outperforming them on seven reasoning benchmarks. Further tuning on open-source instruction datasets enhances performance, showcasing strong generalization abilities. Our approach demonstrates cost-effectiveness compared to human-annotated datasets, offering promising results for future instruction tuning studies.
<!-- Image2 carousel -->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <div class="item">
                        <!-- Displaying an Image of Hybrid Instruction Tuning -->
                        <img src="static/images/teaser.jpg" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" />
                        <h2 class="subtitle" style="font-size: 16px; text-align: center; color: #888888;font-weight: normal;">
                            Figure 2: Comparison between our dataset curation method and previous studies.
                        </h2>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- End image2 carousel -->
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>



<!--end begining introduction-->
<!--begining WEBINSTRUCT-->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Added inline style for right alignment -->
                    <h2 class="title is-3">WEBINSTRUCT</h2>
                    
                    <div class="content has-text-justified">
                        <p>
                            In this section, we outline the process of constructing WEBINSTRUCT from the web corpus. Specif-
                            ically, we divide the data collection pipeline into two stages: (1) high-quality data recall from the
                            web corpus, and (2) Q-A pair extraction and (3) Q-A pair refinement. We depict the full pipeline
                            in Figure 3 and provide an example for extraction and refinement in Figure 4
                        </p>
<!-- Image3 carousel -->
    
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <div class="item">
                        <!-- Displaying an Image of Hybrid Instruction Tuning -->
                        <img src="static\images\combined.jpg" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" />
                        <h2 class="subtitle" style="font-size: 16px; text-align: center; color: #888888;font-weight: normal;">
                            Figure 3: Step 1: Recall relevant documents from Common Crawl. Step 2: Extracting Q-A pairs.
Step 3: Refine with the extracted Q-A pairs.
                        </h2>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- End image3 carousel -->
                    </div>
                    <h3 class="subtitle" style="font-size: 1.2em; text-align: left; color: black;">Recall from Common Crawl</h3>
                    <div class="content has-text-justified">
                        <p>
                            To ensure diversity in our training data across various disciplines like math, science, and engineering, we propose crawling exam problems from educational websites such as stemez.com, homeworkstudy.com, and khanacademy.org. We collected 100K diverse seed examples and randomly selected 100K negative documents from Common Crawl (CC) for training a fastText model. Using the open-source fastText library with 256-dimensional vectors, we trained the model for 3 epochs with a learning rate of 0.1, allowing n-grams up to length 3, and capping word occurrences at 3.
                        </p>
                        <p>
In the initial stage, the trained fastText model recalls the top 100B documents from CC, categorizing them by domain (root URL). We employ GPT-4 to identify domains likely to contain instructional content, achieving satisfactory results through in-context learning. Subsequently, we sample additional documents from these selected domains as positive examples and use documents from non-selected domains and the general CC as negative examples to refine the fastText classifier. The updated classifier then recalls the top 18M documents for further processing.
                        </p>
                        <p>
                            <!-- Image5 carousel -->
                                
                            <section class="hero">
                                <div class="hero-body">
                                    <div class="container is-max-desktop">
                                        <div class="columns is-centered">
                                            <div class="column is-four-fifths">
                                                <div class="item">
                                                    <!-- Displaying an Image of Hybrid Instruction Tuning -->
                                                    <img src="static\images\url.jpg" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" />
                                                    <h2 class="subtitle" style="font-size: 16px; text-align: center; color:#888888;font-weight: normal;">
                                                        Figure 5: The distribution of the top 25 URLs among the 81 total URLs in our instruction dataset.
Most of the instruction data comes from tutoring websites, forums, and homework websites.
                                                    </h2>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </section>
                            
                            <!-- End image5 carousel -->
                            
                            <p>      
                    </div>
                        <h3 class="subtitle" style="font-size: 1.2em; text-align: left; color: black;">Q-A Pair Extraction</h3>

                        <div class="content has-text-justified">
                            <p>
                                Recalled documents contain diverse content from forums, homework, quizzes, and exams. Despite noise like ads and HTML, they contain valuable Q&A pairs. To extract useful content, we preprocess by parsing HTML to remove unrelated info. We then use Mixtral-8Ã—7B to identify Q&A pairs, resulting in 5M candidates. However, many lack explanations. To improve quality, we refine further.We filter out web pages containing questions or answers to evaluation benchmarks to avoid contamination.
                            </p> 
                        </div>

                        <h3 class="subtitle" style="font-size: 1.2em; text-align: left; color: black;">Q-A Pair Refinement</h3>
                        <div class="content has-text-justified">
                            <p>
                                To further improve extracted Q-A pair candidates, we propose to refine them with LLMs. In this
step, we prompt Mixtral-7BÃ—8 and Qwen-72B to reformat the
extracted Q-A pairs. If the answer does not contain any explanation, we prompt the LLMs to complete
the intermediate reasoning steps leading to the answer. We adopt two models to increase diversity.
Eventually, we harvest 10M Q-A pairs as our final instruction-tuning dataset WEBINSTRUCT.
                            </p>
                        </div>
                        <div>
        <!-- Image4 carousel -->
            
        <section class="hero">
            <div class="hero-body">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column is-four-fifths">
                            <div class="item">
                                <!-- Displaying an Image of Hybrid Instruction Tuning -->
                                <img src="static\images\example.jpg" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" />
                                <h2 class="subtitle" style="font-size: 16px; text-align: center; color:#888888;font-weight: normal;">
                                    Figure 4: An illustrating example from WEBINSTRUCT for the extraction and refine step
                                </h2>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <!-- End image4 carousel -->
                        </div>
                        <h3 class="subtitle" style="font-size: 1.2em; text-align: left; color: black;">Dataset Statistics</h3>
                        <div class="content has-text-justified">
                            <p>
                                In Table 1, existing datasets are compared with WEBINSTRUCT. Most SFT datasets contain less than 1M samples of very high quality. XwinMath and OpenMathInstruct are the largest datasets, surpassing 1M samples through GPT-4 synthesis. However, they suffer from narrow domain coverage, mainly based on GSM and MATH. This results in over-fitting to these benchmarks, as shown in Table 2. On the other hand, CT datasets, often sourced from the web, are much larger, exceeding 10B tokens and even reaching 120B tokens, but are costlier to train and have a higher noise ratio. WEBINSTRUCT strikes a balance between scalability and quality, nearing the scalability of CT datasets while maintaining high quality through a three-step construction pipeline. This uniqueness sets it apart from other alternatives.
                            </p>
<!--table1-->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered is-fifths-fifths">
                    <h2 class="title is-3"></h2>
                    <div class="content has-text-justified">
                        <table id="myTable">
                          
                            </thead>
                                 
                                <tr>
                                    <th>Dataset</th>
                                    <th>#Pairs</th>
                                    <th>Domain</th>
                                    <th>Format</th>
                                    <th>Dataset Source</th>
                                </tr>

                            </thead>
                                <tbody>
                                  <tr>
                                    <td>FLAN V2</td>
                                    <td>100K</td>
                                    <td>General</td>
                                    <td>SFT</td>
                                    <td>NLP data + Human CoT</td>
                                  </tr>
                                  <tr>
                                    <td>Self-Instruct</td>
                                    <td>82K</td>
                                    <td>General</td>
                                    <td>SFT</td>
                                    <td>Generated by GPT3</td>
                                  </tr>
                                  <tr>
                                    <td>GPT4-Alpaca</td>
                                    <td>52K</td>
                                    <td>General</td>
                                    <td>SFT</td>
                                    <td>Generated by GPT4</td>
                                  </tr>
                                  <tr>
                                    <td>SuperNI</td>
                                    <td>96K</td>
                                    <td>General</td>
                                    <td>SFT</td>
                                    <td>NLP Datasets</td>
                                  </tr>
                                  <tr>
                                    <td>Tora</td>
                                    <td>16K</td>
                                    <td>Math</td>
                                    <td>SFT</td>
                                    <td>GPT4 GSM+MATH Synthesis</td>
                                  </tr>
                                  <tr>
                                    <td>WizardMath</td>
                                    <td>96K</td>
                                    <td>Math</td>
                                    <td>SFT</td>
                                    <td>GPT4 GSM+MATH Synthesis</td>
                                  </tr>
                                  <tr>
                                    <td>MathInstruct</td>
                                    <td>262K</td>
                                    <td>Math</td>
                                    <td>SFT</td>
                                    <td>GPT4 Math datasets Synthesis</td>
                                  </tr>
                                  <tr>
                                    <td>MetaMathQA</td>
                                    <td>395K</td>
                                    <td>Math</td>
                                    <td>SFT</td>
                                    <td>GPT4 GSM+MATH Synthesis</td>
                                  </tr>
                                  <tr>
                                    <td>XwinMath</td>
                                    <td>1.4M</td>
                                    <td>Math</td>
                                    <td>SFT</td>
                                    <td>GPT4 GSM+MATH Synthesis</td>
                                  </tr>
                                  <tr>
                                    <td>OpenMathInstruct</td>
                                    <td>1.8M</td>
                                    <td>Math</td>
                                    <td>SFT</td>
                                    <td>GPT4 GSM+MATH Synthesis</td>
                                  </tr>
                            </tbody>
                        </table>

                        <table id="myTable">
                            <thead>
                                <tr>
                                    <th>Dataset</th>
                                    <th>#Tokens</th>
                                    <th>Domain</th>
                                    <th>Format</th>
                                    <th>Dataset Source</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>OpenWebMath</td>
                                    <td>12B</td>
                                    <td>Math</td>
                                    <td>LM</td>
                                    <td>Filtered from Web</td>
                                </tr>
                                <tr>
                                    <td>MathPile</td>
                                    <td>10B</td>
                                    <td>Math</td>
                                    <td>LM</td>
                                    <td>Filtered from Web</td>
                                </tr>
                                <tr>
                                    <td>Cosmopeida</td>
                                    <td>25B</td>
                                    <td>General</td>
                                    <td>LM</td>
                                    <td>Synthesized by Mixtral</td>
                                </tr>
                                <tr>
                                    <td>MINERVA</td>
                                    <td>38B</td>
                                    <td>Math</td>
                                    <td>LM</td>
                                    <td>Filtered from Web</td>
                                </tr>
                                <tr>
                                    <td>Proof-Pile-2</td>
                                    <td>55B</td>
                                    <td>Math</td>
                                    <td>LM</td>
                                    <td>OWM+Arxiv+Code</td>
                                </tr>
                                <tr>
                                    <td>Galactica</td>
                                    <td>106B</td>
                                    <td>Math & Sci.</td>
                                    <td>LM</td>
                                    <td>Filtered from Web</td>
                                </tr>
                                <tr>
                                    <td>DeepseekMath</td>
                                    <td>120B</td>
                                    <td>Math</td>
                                    <td>LM</td>
                                    <td>Recalled from Web</td>
                                </tr>
                                <tr>
                                    <td>WEBINSTRUCT</td>
                                    <td>(10M) 5B</td>
                                    <td>Math & Sci.</td>
                                    <td>SFT</td>
                                    <td> Recall and Extracted from Web</td>
                                </tr>
                            </tbody>

                            

                        </table>
                        <capion>Table 1: The list of existing supervise-fine-tuning and continue-training datasets. The SFT datasets
                            are mostly from NLP datasets or completely synthesized by GPT-4. The CT datasets are much larger
                            because they are filtered or recalled from the web. The the content contains lots of noise. We are the
                            first dataset to combine these two to build high-quality yet large-scale SFT dataset.</caption>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
<!--table1-->
<!--table2-->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered is-fifths-fifths">
                    <h2 class="title is-3"></h2>
                    <div class="content has-text-justified">
                        <table id="myTable">
                            <thead>
                                <tr>
                                    <th>Model</th>
                                    <th>TheoremQA</th>
                                    <th>MATH</th>
                                    <th>GSM8K</th>
                                    <th>GPQA</th>
                                    <th>MMLU-ST</th>
                                    <th>BBH</th>
                                    <th>ARC-C</th>
                                    <th>Avg</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>GPT-4-Turbo-0409</td>
                                    <td>48.4</td>
                                    <td>69.2</td>
                                    <td>94.5</td>
                                    <td>46.2</td>
                                    <td>76.5</td>
                                    <td>86.7</td>
                                    <td>93.6</td>
                                    <td>73.6</td>
                                </tr>
                                <tr>
                                    <td colspan="9" style="text-align:center"><strong>Parameter Size between 20B and 110B</strong></td>
                                </tr>
                                <tr>
                                    <td>Qwen-1.5-110B</td>
                                    <td><u>34.9</u></td>
                                    <td><u>49.6</u></td>
                                    <td><u>85.4</u></td>
                                    <td>35.9</td>
                                    <td><u>73.4</u></td>
                                    <td><u>74.8</u></td>
                                    <td>91.6</td>
                                    <td><u>63.6</u></td>
                                </tr>
                                <tr>
                                    <td>Qwen-1.5-72B</td>
                                    <td>29.3</td>
                                    <td>46.8</td>
                                    <td>77.6</td>
                                    <td><u>36.3</u></td>
                                    <td>68.5</td>
                                    <td>68.0</td>
                                    <td><u>92.2</u></td>
                                    <td>59.8</td>
                                </tr>
                                <tr>
                                    <td>Deepseek-LM-67B</td>
                                    <td>25.3</td>
                                    <td>15.9</td>
                                    <td>66.5</td>
                                    <td>31.8</td>
                                    <td>57.4</td>
                                    <td>71.7</td>
                                    <td>86.8</td>
                                    <td>50.7</td>
                                </tr>
                                <tr>
                                    <td><u>Yi-34B</u></td>
                                    <td>23.2</td>
                                    <td>15.9</td>
                                    <td>67.9</td>
                                    <td>29.7</td>
                                    <td>62.6</td>
                                    <td>66.4</td>
                                    <td>89.5</td>
                                    <td>50.7</td>
                                </tr>
                                <tr>
                                    <td>Llemma-34B</td>
                                    <td>21.1</td>
                                    <td>25.0</td>
                                    <td>71.9</td>
                                    <td>29.2</td>
                                    <td>54.7</td>
                                    <td>48.4</td>
                                    <td>69.5</td>
                                    <td>45.7</td>
                                </tr>
                                <tr>
                                    <td><u>Mixtral-8&times;7B</u></td>
                                    <td>23.2</td>
                                    <td>28.4</td>
                                    <td>74.4</td>
                                    <td>29.7</td>
                                    <td>59.7</td>
                                    <td>66.8</td>
                                    <td>84.7</td>
                                    <td>52.4</td>
                                </tr>
                                <tr>
                                    <td>Mixtral-8&times;7B-Instruct</td>
                                    <td>25.3</td>
                                    <td>22.1</td>
                                    <td>71.7</td>
                                    <td>32.4</td>
                                    <td>61.4</td>
                                    <td>57.3</td>
                                    <td>84.7</td>
                                    <td>50.7</td>
                                </tr>
                                <tr>
                                    <td>Intern-Math-20B</td>
                                    <td>17.1</td>
                                    <td>37.7</td>
                                    <td>82.9</td>
                                    <td>28.9</td>
                                    <td>50.1</td>
                                    <td>39.3</td>
                                    <td>68.6</td>
                                    <td>46.4</td>
                                </tr>
                                <tr>
                                    <td colspan="9" style="text-align:center"><strong>Trained only with WEBINSTRUCT (All evaluations are held-out)</strong></td>
                                <tr> 
                                    <tr>
                                        <td>MAmmoTH2-34B</td>
                                        <td>30.4</td>
                                        <td>35.0</td>
                                        <td>75.6</td>
                                        <td>31.8</td>
                                        <td>64.5</td>
                                        <td>68.0</td>
                                        <td>90.0</td>
                                        <td>56.4</td>
                                      </tr>
                                      <tr bgcolor="#E0FFFF">
                                        <td>âˆ† over <u>Yi</u></td>
                                        <td>+7.2</td>
                                        <td>+19.1</td>
                                        <td>+7.7</td>
                                        <td>+2.1</td>
                                        <td>+2.9</td>
                                        <td>+1.2</td>
                                        <td>+0.5</td>
                                        <td>+5.8</td>
                                      </tr>
                                      <tr>
                                        <td>MAmmoTH2-8x7B</td>
                                        <td>32.2</td>
                                        <td>39.0</td>
                                        <td>75.4</td>
                                        <td>36.8</td>
                                        <td>67.4</td>
                                        <td>71.1</td>
                                        <td>87.5</td>
                                        <td>58.9</td>
                                      </tr>
                                      <tr bgcolor="#E0FFFF">
                                        <td>âˆ† over <u>Mixtral</u></td>
                                        <td>+9.2</td>
                                        <td>+10.6</td>
                                        <td>+1.0</td>
                                        <td>+7.1</td>
                                        <td>+7.4</td>
                                        <td>+3.3</td>
                                        <td>+2.8</td>
                                        <td>+6.5</td>
                                      </tr>
                                    <tr>
                                        <td colspan="9" style="text-align:center"><strong>Continue trained with additional instruction datasets (All held-out except MATH and GSM8K</strong></td>
                                    <tr>  
                                        <tr>
                                            <td>MAmmoTH2-8x7B-Plus</td>
                                            <td><strong>34.1</strong></td>
                                            <td><strong>47.0</strong></td>
                                            <td><strong>86.4</strong></td>
                                            <td><strong>37.4</strong></td>
                                            <td><strong>72.4</strong></td>
                                            <td><strong>74.1</strong></td>
                                            <td><strong>88.4</strong></td>
                                            <td><strong>62.8</strong></td>
                                          </tr>
                                          <tr bgcolor="#E0FFFF">
                                            <td>âˆ† over Qwen-1.5-110B</td>
                                            <td>-0.8</td>
                                            <td>-2.6</td>
                                            <td>+1.0</td>
                                            <td>+2.5</td>
                                            <td>-1.0</td>
                                            <td>-0.7</td>
                                            <td>-4.0</td>
                                            <td>-0.8</td>
                                          </tr>
                                        <tr>
                                            <td colspan="9" style="text-align:center"><strong>Parameter Size = 7B or 8B</strong></td>
                                        <tr>
                                            <tr>
                                                <td>Deepseek-7B</td>
                                                <td>15.7</td>
                                                <td>6.4</td>
                                                <td>17.4</td>
                                                <td>25.7</td>
                                                <td>43.1</td>
                                                <td>42.8</td>
                                                <td>47.8</td>
                                                <td>28.4</td>
                                              </tr>
                                              <tr>
                                                <td>Qwen-1.5-7B</td>
                                                <td>14.2</td>
                                                <td>13.3</td>
                                                <td>54.1</td>
                                                <td>26.7</td>
                                                <td>45.4</td>
                                                <td>45.2</td>
                                                <td>75.6</td>
                                                <td>39.2</td>
                                              </tr>
                                              <tr>
                                                <td><u>Mistral-7B</u></td>
                                                <td>19.2</td>
                                                <td>11.2</td>
                                                <td>36.2</td>
                                                <td>24.7</td>
                                                <td>50.1</td>
                                                <td>55.7</td>
                                                <td>74.2</td>
                                                <td>38.8</td>
                                              </tr>
                                              <tr>
                                                <td>Gemma-7B</td>
                                                <td>21.5</td>
                                                <td>24.3</td>
                                                <td>46.4</td>
                                                <td>25.7</td>
                                                <td>53.3</td>
                                                <td>57.4</td>
                                                <td>72.5</td>
                                                <td>43.0</td>
                                              </tr>
                                              <tr>
                                                <td>Llemma-7B</td>
                                                <td>17.2</td>
                                                <td>18.0</td>
                                                <td>36.4</td>
                                                <td>23.2</td>
                                                <td>45.2</td>
                                                <td>44.9</td>
                                                <td>50.5</td>
                                                <td>33.6</td>
                                              </tr>
                                              <tr>
                                                <td>WizardMath-7B-1.1</td>
                                                <td>11.7</td>
                                                <td>33.0</td>
                                                <td>83.2</td>
                                                <td>28.7</td>
                                                <td>52.7</td>
                                                <td>56.7</td>
                                                <td>76.9</td>
                                                <td>49.0</td>
                                              </tr>
                                              <tr>
                                                <td>OpenMath-Mistral</td>
                                                <td>13.1</td>
                                                <td>9.1</td>
                                                <td>24.5</td>
                                                <td>26.5</td>
                                                <td>43.7</td>
                                                <td>49.5</td>
                                                <td>69.4</td>
                                                <td>33.7</td>
                                              </tr>
                                            <tr>
                                              <td>Abel-7B-002</td>
                                              <td>19.3</td>
                                              <td>29.5</td>
                                              <td>83.2</td>
                                              <td>30.3</td>
                                              <td>29.7</td>
                                              <td>32.7</td>
                                              <td>72.5</td>
                                              <td>42.5</td>
                                            </tr>
                                            <tr>
                                              <td>Intern-Math-7B</td>
                                              <td>13.2</td>
                                              <td>34.6</td>
                                              <td>78.1</td>
                                              <td>22.7</td>
                                              <td>41.1</td>
                                              <td>48.1</td>
                                              <td>59.8</td>
                                              <td>42.5</td>
                                            </tr>
                                            <tr>
                                              <td>Rho-1-Math-7B</td>
                                              <td>21.0</td>
                                              <td>31.0</td>
                                              <td>66.9</td>
                                              <td>29.2</td>
                                              <td>53.1</td>
                                              <td>57.7</td>
                                              <td>72.7</td>
                                              <td>47.3</td>
                                            </tr>
                                            <tr >
                                              <td>Deepseek-Math-7B</td>
                                              <td>25.3</td>
                                              <td>34.0</td>
                                              <td>64.2</td>
                                              <td>29.2</td>
                                              <td>56.4</td>
                                              <td>59.5</td>
                                              <td>67.8</td>
                                              <td>48.0</td>
                                            </tr>
                                            <tr>
                                              <td>Deepseek-Math-Instruct</td>
                                              <td>23.7</td>
                                              <td>44.3</td>
                                              <td>82.9</td>
                                              <td>31.8</td>
                                              <td>59.3</td>
                                              <td>55.4</td>
                                              <td>70.1</td>
                                              <td>52.5</td>
                                            </tr>
                                            <tr>
                                              <td><u>Llama-3-8B</u></td>
                                              <td>20.1</td>
                                              <td>21.3</td>
                                              <td>54.8</td>
                                              <td>27.2</td>
                                              <td>55.6</td>
                                              <td>61.1</td>
                                              <td>78.6</td>
                                              <td>45.5</td>
                                            </tr>
                                            <tr>
                                              <td>Llama-3-8B-Instruct</td>
                                              <td>22.8</td>
                                              <td>30.0</td>
                                              <td>79.5</td>
                                              <td>34.5</td>
                                              <td>60.2</td>
                                              <td>66.0</td>
                                              <td>80.8</td>
                                              <td>53.4</td>
                                            </tr>
                                            <tr>
                                                <td colspan="9" style="text-align:center"><strong>Trained only with WEBINSTRUC (All evaluations are held-out)</strong></td>
                                            <tr>
                                                <tr>
                                                    <td>MAmmoTH2-7B</td>
                                                    <td>26.7</td>
                                                    <td>34.2</td>
                                                    <td>67.4</td>
                                                    <td>34.8</td>
                                                    <td>60.6</td>
                                                    <td>60.0</td>
                                                    <td>81.8</td>
                                                    <td>52.2</td>
                                                  </tr>
                                                  <tr bgcolor="#E0FFFF">
                                                    <td>âˆ† over <u>Mistral</u></td>
                                                    <td>+7.5</td>
                                                    <td>+23.0</td>
                                                    <td>+31.2</td>
                                                    <td>+10.1</td>
                                                    <td>+10.5</td>
                                                    <td>+4.3</td>
                                                    <td>+7.6</td>
                                                    <td>+13.4</td>
                                                  </tr>
                                                  <tr>
                                                    <td>MAmmoTH2-8B</td>
                                                    <td>29.7</td>
                                                    <td>33.4</td>
                                                    <td>67.9</td>
                                                    <td>38.4</td>
                                                    <td>61.0</td>
                                                    <td>60.8</td>
                                                    <td>81.0</td>
                                                    <td>53.1</td>
                                                  </tr>
                                                  <tr bgcolor="#E0FFFF">
                                                    <td>âˆ† over <u>Llama3</u></td>
                                                    <td>+9.6</td>
                                                    <td>+12.1</td>
                                                    <td>+13.1</td>
                                                    <td>+11.2</td>
                                                    <td>+5.4</td>
                                                    <td>-0.3</td>
                                                    <td>+2.4</td>
                                                    <td>+7.6</td>
                                                  </tr>    
                                                <tr>
                                                   <td colspan="9" style="text-align:center"><strong>Continue trained with additional instruction datasets (All held-out except MATH and GSM8K)</strong></td>
                                                 <tr>
                                                    <tr>
                                                        <td>MAmmoTH2-7B-Plus</td>
                                                        <td>29.2</td>
                                                        <td><strong>45.0</strong></td>
                                                        <td><strong>84.7</strong></td>
                                                        <td>30.3</td>
                                                        <td>64.5</td>
                                                        <td>63.1</td>
                                                        <td>83.0</td>
                                                        <td>57.1</td>
                                                      </tr>
                                                      <tr>
                                                        <td>MAmmoTH2-8B-Plus</td>
                                                        <td><strong>32.5</strong></td>
                                                        <td>42.8</td>
                                                        <td>84.1</td>
                                                        <td><strong>37.3</strong></td>
                                                        <td><strong>65.7</strong></td>
                                                        <td><strong>67.8</strong></td>
                                                        <td><strong>83.4</strong></td>
                                                        <td><strong>59.1</strong></td>
                                                      </tr>
                                                      <tr bgcolor="#E0FFFF">
                                                        <td>âˆ† over best baseline</td>
                                                        <td>+7.2</td>
                                                        <td>+0.7</td>
                                                        <td>+1.5</td>
                                                        <td>+2.8</td>
                                                        <td>+5.5</td>
                                                        <td>+1.8</td>
                                                        <td>+2.6</td>
                                                        <td>+5.7</td>
                                                      </tr>
                                                    </table>    



                            </tbody>
                            <capion>Table 2: Our main results on various science reasoning datasets. All the models without â€˜-Instructâ€™
                                refers to the released base model before instruction tuning. For the experimental results, if they are
                                reported by official paper or OpenCompass, we take the reported numbers. If not, we will use our
                                own script for evaluation. Underscored results are the best baseline scores under the size constraint.</caption>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
<!--table2-->
                        </div>
                </div>
            </div>
        </div>
    </div>
</section>


<!-- Image6 carousel -->
                                
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <div class="item">
                        <!-- Displaying an Image of Hybrid Instruction Tuning -->
                        <img src="static\images\Figure_1.jpg" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" />
                        <h2 class="subtitle" style="font-size: 16px; text-align: center; color:#888888;font-weight: normal;">
                            Figure 6: Mistral-7B model performance improves with scaling instructions. Additionally, SFT Loss
                            is a more effective learning approach compared to LM Loss.
                        </h2>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- End image6 carousel -->




<!--ending WEBINSTRUCT-->

   

<!-- 
    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Overall Results</h2>
                        <div class="item">
                            Your image here -->
                            <!-- <img src="static/images/overall_results.png" alt="MY ALT TEXT" />
                            <h2 class="subtitle has-text-centered">
                                Figure 2: Overall results of ðŸ¦£MAmmoTH on the in-domain and out-of-domain datasets.
                            </h2>
                            <p>
                                Overall, we can see that MAmmoTH and MAmmoTH-Coder are able to outperform the SoTA model at different scales. In general, the performance gain for OOD datasets is more significant than IND datasets. These results show us the potential of our models as
                                a mathematical generalist. On several datasets, MAmmoTH-Coder-34B and MAmmoTH-70B are even surpassing closed-source LLMs (see more break down results below).
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section> -->


    <!-- <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Where does the gain come from?</h2>
                        <div class="item"> -->
                            <!-- Your image here -->
                            <!-- <img src="static/images/ablation_results.png" alt="MY ALT TEXT" />
                            <h2 class="subtitle">
                                Figure 3: Investigation of the influence of CoT \& PoT hybrid training on the 7B Llama-2 model. Key insights include: 1) The SoTA model, utilizing dataset-specific CoT fine-tuning on GSM and MATH, displays strong performance within its domains but struggles
                                in OOD scenarios; 2) Diverse data sources in MathInstruct enable better math generalist model; 3) Fine-tuning on the PoT subsets generally outperforms fine-tuning on the CoT subsets; 4) Hybrid training yields the best-performing
                                model.
                            </h2>
                            <p>
                                In order to better understand what factors contribute to the great gain of ðŸ¦£MAmmoTH over existing baselines, we set up a group of control experiments in the Figure 3. We study the following setups:
                                <ol>
                                    <li>ðŸ¦£<b>MAmmoTH (MathInstruct - CoT):</b> This experiment aims to understand how much our curated CoT data could improve the generalization over the SoTA model WizardMath trained specifically on GSM + MATH. As can be seen,
                                        while sacrificing accuracy on GSM + MATH by 3%, our CoT subset fine-tuning improves the overall nine-dataset accuracy from 27% to 32%. </li>
                                    <li>ðŸ¦£<b>MAmmoTH (MathInstruct - PoT):</b> This experiment aims to understand the advantage of our PoT subset. As can be observed, our PoT subset fine-tuning can significantly improve the overall accuracy from 27% to 37.5%.
                                        This ablation reflects the importance of unlocking the program generation capabilities of our model.</li>
                                    <li>ðŸ¦£<b>MAmmoTH (MathInstruct - Hybrid):</b> We further combine CoT and PoT as the hybrid training data to achieve the best overall performance of 45.4%. This combined gain comes from two aspects:
                                        <ul style="list-style-type: disc;">
                                            <li>
                                                The CoT subset can help maintain the generic language-based reasoning skills to handle scenarios where PoT cannot handle well, e.g., the multi-choice questions in AQuA, SAT, and MMLU.
                                            </li>
                                            <li>
                                                The PoT subset can teach the model how to utilize Python APIs to solve complex math problems with high precision, e.g., the MATH problems requiring complex computation.
                                            </li>
                                        </ul>
                                    </li>
                                </ol> -->





                                <!-- We put some case studies in Appendix \ref{sec:case_study} to demonstrate the respective advantages of PoT and CoT in solving different types of math problems. To summarize, we attribute our substantial gain to: 1) diverse data sources covering different math fields and complexity levels and 2) a hybrid of CoT \& PoT instruction tuning strategy.  -->

                            <!-- </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section> -->




<!--table3-->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered is-fifths-fifths">
                    <h2 class="title is-3"></h2>
                    <div class="content has-text-justified">
                        <table id="myTable">
                            <thead>
                                <tr>
                                    <th></th>
                                    <th>HumanEval</th>
                                    <th>HumanEval+</th>
                                    <th>MBPP</th>
                                    <th>MBPP+</th>
                                    <th>Average</th>
                                    <th>Average+</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Mistral-7B</td>
                                    <td>28.7</td>
                                    <td>23.8</td>
                                    <td>51.9</td>
                                    <td>42.1</td>
                                    <td>40.3</td>
                                    <td>33.0</td>
                                </tr>
                                <tr>
                                    <td>Gemma-7B</td>
                                    <td>26.8</td>
                                    <td>20.1</td>
                                    <td>52.6</td>
                                    <td>43.4</td>
                                    <td>39.7</td>
                                    <td>31.8</td>
                                </tr>
                                <tr>
                                    <td>Llama-3-8B</td>
                                    <td>33.5</td>
                                    <td>29.3</td>
                                    <td>61.4</td>
                                    <td>51.6</td>
                                    <td>47.5</td>
                                    <td>40.5</td>
                                </tr>
                                <tr>
                                    <td>Gemma-1.1-7B-Instruct</td>
                                    <td>42.7</td>
                                    <td>35.4</td>
                                    <td>57.1</td>
                                    <td>45.0</td>
                                    <td>49.9</td>
                                    <td>40.2</td>
                                </tr>
                                <tr>
                                    <td>Mistral-7B-Instruct-v0.2</td>
                                    <td><strong>75.0</strong></td>
                                    <td><strong>70.1</strong></td>
                                    <td>44.7</td>
                                    <td>37.0</td>
                                    <td>59.9</td>
                                    <td>53.6</td>
                                </tr>
                                <tr>
                                    <td>Llama-3-8B-Instruct</td>
                                    <td>61.6</td>
                                    <td>56.7</td>
                                    <td><strong>70.1</strong></td>
                                    <td><strong>59.3</strong></td>
                                    <td><u>65.9</u></td>
                                    <td><u>58.0</u></td>
                                </tr>
                                <tr>
                                    <td>Mixtral-8&times;7B-Instruct-v0.1</td>
                                    <td>45.1</td>
                                    <td>39.6</td>
                                    <td>59.5</td>
                                    <td>49.7</td>
                                    <td>52.3</td>
                                    <td>44.7</td>
                                </tr>
                                <tr>
                                 <tr>   
                                <tr>
                                    <td>MAmmoTH2-7B-Plus</td>
                                    <td><u>72.1</u></td>
                                    <td><u>65.9</u></td>
                                    <td>60.1</td>
                                    <td>50.4</td>
                                    <td><strong>66.1</strong></td>
                                    <td><strong>58.2</strong></td>
                                </tr>
                                <tr>
                                    <td>MAmmoTH2-8B-Plus</td>
                                    <td>63.4</td>
                                    <td>57.9</td>
                                    <td>60.4</td>
                                    <td>48.6</td>
                                    <td>61.9</td>
                                    <td>53.3</td>
                                </tr>
                                <tr>
                                    <td>MAmmoTH2-8x7B-Plus</td>
                                    <td>57.9</td>
                                    <td>53.7</td>
                                    <td><u>68.7</u></td>
                                    <td><u>56.9</u></td>
                                    <td>63.3</td>
                                    <td>55.3</td>
                                </tr>
                            </tbody>
                            
                        </table>
                        <capion>Table 3:Code generation results of different models. Baseline results are copied from the
                                EvalPlus leaderboard.</caption>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!--table3-->
<!--table4-->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered is-fifths-fifths">
                    <h2 class="title is-3"></h2>
                    <div class="content has-text-justified">
                        <table id="myTable">
                            <thead>
                                <tr>
                                    <th></th>
                                    <th>MT-Bench</th>
                                    <th>AlpacaEval 2.0</th>
                                    <th>Arena Hard</th>
                                    <th>MMLU</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>GPT-4-1106-preview</td>
                                    <td>9.32</td>
                                    <td>50.0</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>GPT-3.5-Turbo-1106</td>
                                    <td>8.32</td>
                                    <td>19.3</td>
                                    <td>18.9</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td>GPT-3.5-Turbo-0301</td>
                                    <td>7.94</td>
                                    <td>18.1</td>
                                    <td>18.1</td>
                                    <td>70.0</td>
                                </tr>
                                <tr>
                                    <td>Tulu-2-DPO-70B</td>
                                    <td>7.89</td>
                                    <td>21.2</td>
                                    <td>15.0</td>
                                    <td>67.8</td>
                                </tr>
                                <tr>
                                    <td>Llama-2-70b-chat</td>
                                    <td>6.86</td>
                                    <td>14.7</td>
                                    <td>11.6</td>
                                    <td>63.0</td>
                                </tr>
                                <tr>
                                    <td>Yi-34B-Chat</td>
                                    <td>7.86</td>
                                    <td>27.2</td>
                                    <td>23.1</td>
                                    <td>73.5</td>
                                </tr>
                                <tr>
                                    <td>Gemma-1.1-7B-Instruct</td>
                                    <td>-</td>
                                    <td>10.4</td>
                                    <td>7.5</td>
                                    <td>64.3</td>
                                </tr>
                                <tr>
                                    <td>Mistral-7B-Instruct-v0.2</td>
                                    <td>7.60</td>
                                    <td>17.1</td>
                                    <td>12.6</td>
                                    <td>60.8</td>
                                </tr>
                                <tr>
                                    <td>Llama-3-8B-Instruct</td>
                                    <td>8.02</td>
                                    <td>22.9</td>
                                    <td>20.6</td>
                                    <td>67.2</td>
                                </tr>
                                <tr>
                                    <td>Mixtral-8&times;7B-Instruct-v0.1</td>
                                    <td><strong>8.30</strong></td>
                                    <td>23.7</td>
                                    <td>23.4</td>
                                    <td><strong>70.6</strong></td>
                                </tr>
                                <tr>
                                    <td>MAmmoTH2-7B-Plus</td>
                                    <td>7.88</td>
                                    <td>23.4</td>
                                    <td>14.6</td>
                                    <td>63.3</td>
                                </tr>
                                <tr>
                                    <td>MAmmoTH2-8B-Plus</td>
                                    <td>7.95</td>
                                    <td>18.5</td>
                                    <td>16.6</td>
                                    <td>64.6</td>
                                </tr>
                                <tr>
                                    <td>MAmmoTH2-8x7B-Plus</td>
                                    <td><u>8.20</u></td>
                                    <td><strong>33.8</strong></td>
                                    <td><strong>32.6</strong></td>
                                    <td><u>68.3</u></td>
                                </tr>
                            </tbody>
                        </table>
                        <capion>Table 4: Evaluation of instruction-following and MMLU performance for various models. Baseline
                            scores are sourced from the original papers or the MT-Bench, AlpacaEval 2.0, and Arena Hard leader-
                            boards. (â€œ-â€) indicates that the score was not available from the referenced sources. MAmmoTH2-Plus
                            exhibits strong general conversational ability and excels at multitask language understanding across a
                            wide range of domains compared to their official instruct counterparts and larger models</caption>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<footer class="footer">
    <!-- <div class="container"> -->
      <div class="content has-text-centered">
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is website adapted from <a href="https://tiger-ai-lab.github.io/MAmmoTH/">MAmmoTH</a> and <a href="https://mmmu.github.io/">MMMU</a>, licensed under a <a rel="license"
                                                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    <!-- </div> -->
  
  </footer>

<!--table4-->
</html>
